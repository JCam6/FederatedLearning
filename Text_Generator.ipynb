{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e8165c-088f-40a5-a681-8dfc3d8bc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1505627c-5135-4d95-b471-45b483546980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Check-OK'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow_federated as tff\n",
    "import functools\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "tff.federated_computation(lambda: 'Check-OK')()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8168f-2e59-414a-a54f-dc87ed46b560",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5a7fc-6542-4cbc-af96-cdf5280c075f",
   "metadata": {},
   "source": [
    "### Generate the vocabulary lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec69fec-4b63-45d7-93ac-1124bc2ddb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "\n",
    "char2idx = {u:1 for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f4b00-afd5-4093-b475-abe733be024a",
   "metadata": {},
   "source": [
    "### Load pre-trained model and generate random text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b324c59a-008e-4cf5-978e-743b1adfc87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(batch_size):\n",
    "    urls = {1: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel', \n",
    "            8: 'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel'}\n",
    "    assert batch_size in urls, 'batch_size must be in ' + str(urls.keys())\n",
    "    url = urls[batch_size]\n",
    "    local_file = tf.keras.utils.get_file(os.path.basename(url), origin=url)\n",
    "    return tf.keras.models.load_model(local_file, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9507ad5-e327-4757-8675-568b5f9bd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # from tensorflow.org/tutorials/sequences/text_generation\n",
    "    num_generate = 200\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id],0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64da26f0-a6f1-4a5f-b9a9-bec53695aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where art thou tensorflow, you ask? ere is he?\"\n",
      "\n",
      "In any name.\n",
      "\n",
      "He was suddenly seee of scarecrows or MissoPulate! Which is he, by money;\n",
      "and far dear father, and it was histoors contended with Notre Manette, received him to the dri\n"
     ]
    }
   ],
   "source": [
    "keras_model_batch1 = load_model(batch_size=1)\n",
    "print(generate_text(keras_model_batch1, 'Where art thou tensorflow, you ask? '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af363d91-f84d-42ac-807f-531543ca3184",
   "metadata": {},
   "source": [
    "## Load Federated dataset and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba5b62f2-6018-40a1-b1d1-823d013051df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204b39b5-0d6d-4468-ad5f-906caf0a6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'', shape=(), dtype=string)\n",
      "tf.Tensor(b'What?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#example look at data\n",
    "raw_example_dataset = train_data.create_tf_dataset_for_client(\n",
    "    'THE_TRAGEDY_OF_KING_LEAR_KING')\n",
    "for x in raw_example_dataset.take(2):\n",
    "    print(x['snippets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580170a8-0155-4eaa-b187-6078ba2a6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pre-processing parameters\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 100  # For dataset shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad897393-2300-42cf-8819-0a170a1c2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a lookup table to map string chars to indexes,\n",
    "# using the vocab loaded above:\n",
    "table = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=vocab, values=tf.constant(list(range(len(vocab))),dtype=tf.int64)),default_value=0)\n",
    "\n",
    "def to_ids(x):\n",
    "  s = tf.reshape(x['snippets'], shape=[1])\n",
    "  chars = tf.strings.bytes_split(s).values\n",
    "  ids = table.lookup(chars)\n",
    "  return ids\n",
    "\n",
    "def split_input_target(chunk):\n",
    "  input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "  target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "  return (input_text, target_text)\n",
    "\n",
    "def preprocess(dataset):\n",
    "  return (\n",
    "      # Map ASCII chars to int64 indexes using the vocab\n",
    "      dataset.map(to_ids)\n",
    "      # Split into individual chars\n",
    "      .unbatch()\n",
    "      # Form example sequences of SEQ_LENGTH +1\n",
    "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "      # Shuffle and form minibatches\n",
    "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "      # And finally split into (input, target) tuples,\n",
    "      # each of length SEQ_LENGTH.\n",
    "      .map(split_input_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36be1d15-ed27-44f3-8bac-171f71fed449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "example_dataset = preprocess(raw_example_dataset)\n",
    "print(example_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804bbb6-a820-4b79-bb91-3537c83155ae",
   "metadata": {},
   "source": [
    "## Compile model and test pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aae9f96-be21-4b8e-83a4-77116066ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "\n",
    "  def __init__(self, name='accuracy', dtype=tf.float32):\n",
    "    super().__init__(name, dtype=dtype)\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = tf.reshape(y_true, [-1, 1])\n",
    "    y_pred = tf.reshape(y_pred, [-1, len(vocab), 1])\n",
    "    return super().update_state(y_true, y_pred, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a181311b-e90a-4f4d-9945-5fa2b14d4b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on an example Shakespeare character: 0.398000\n",
      "Expected accuracy for random guessing: 0.012\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on completely random data: 0.011\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8  # The training and eval batch size for the rest of this tutorial.\n",
    "keras_model = load_model(batch_size=BATCH_SIZE)\n",
    "keras_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[FlattenedCategoricalAccuracy()])\n",
    "\n",
    "# Confirm that loss is much lower on Shakespeare than on random data\n",
    "\n",
    "loss, accuracy = keras_model.evaluate(example_dataset.take(5), verbose=0)\n",
    "print('Evaluating on an example Shakespeare character: {a:3f}'.format(a=accuracy))\n",
    "\n",
    "# As a sanity check, can construct some completely random data, where expect\n",
    "# the accuracy to be essentially random:\n",
    "\n",
    "random_guessed_accuracy = 1.0 / len(vocab)\n",
    "print('Expected accuracy for random guessing: {a:.3f}'.format(a=random_guessed_accuracy))\n",
    "\n",
    "random_indexes = np.random.randint(low=0, high=len(vocab), size=1 * BATCH_SIZE * (SEQ_LENGTH + 1))\n",
    "data = collections.OrderedDict(snippets=tf.constant(''.join(np.array(vocab)[random_indexes]), shape=[1, 1]))\n",
    "random_dataset = preprocess(tf.data.Dataset.from_tensor_slices(data))\n",
    "loss, accuracy = keras_model.evaluate(random_dataset, steps=10, verbose=0)\n",
    "print('Evaluating on completely random data: {a:.3f}'.format(a=accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa854244-c6fa-40bd-a43c-d7e479307206",
   "metadata": {},
   "source": [
    "## Fine-tune model with FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1314f024-6dd6-49d5-88da-0ebe19b56c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the keras_model inside `create_tff_model()`, which TFF will\n",
    "# call to produce a new copy of the model inside the graph that it will \n",
    "# serialize. Note: want to construct all the necessary objects needed\n",
    "# _inside_ this method.\n",
    "\n",
    "def create_tff_model():\n",
    "  # TFF uses an `input_spec` so it knows the types and shapes\n",
    "  # that model expects.\n",
    "  input_spec = example_dataset.element_spec\n",
    "  keras_model_clone = tf.keras.models.clone_model(keras_model)\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model_clone,\n",
    "      input_spec=input_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[FlattenedCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f200f85-dcfb-4e3b-b5c3-2fee05d3611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['gru_1/gru_cell/kernel:0', 'gru_1/gru_cell/recurrent_kernel:0', 'gru_1/gru_cell/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    }
   ],
   "source": [
    "# This command builds all the TensorFlow graphs and serializes them: \n",
    "fed_avg = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=create_tff_model,client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "044a9889-99d2-4336-b1d0-d4f49969bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=4.399, accuracy=0.142\n"
     ]
    }
   ],
   "source": [
    "state = fed_avg.initialize()\n",
    "state, metrics = fed_avg.next(state, [example_dataset.take(5)])\n",
    "train_metrics = metrics['train']\n",
    "print('loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278710f3-01ab-458b-821d-e0c24a446ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(client, source=train_data):\n",
    "  return preprocess(source.create_tf_dataset_for_client(client)).take(5)\n",
    "\n",
    "clients = ['ALL_S_WELL_THAT_ENDS_WELL_CELIA', 'MUCH_ADO_ABOUT_NOTHING_OTHELLO',]\n",
    "\n",
    "train_datasets = [data(client) for client in clients]\n",
    "\n",
    "# concatenate the test datasets for evaluation with Keras by creating a \n",
    "# Dataset of Datasets, and then identity flat mapping across all the examples.\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices([data(client, test_data) for client in clients]).flat_map(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcf7aad1-8ab6-4e21-be85-07ee8e08329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "\tEval: loss=3.304, accuracy=0.416\n",
      "\tTrain: loss=4.304, accuracy=0.094\n",
      "Round 1\n",
      "\tEval: loss=4.255, accuracy=0.162\n",
      "\tTrain: loss=4.134, accuracy=0.232\n",
      "Round 2\n",
      "\tEval: loss=4.115, accuracy=0.168\n",
      "\tTrain: loss=3.987, accuracy=0.242\n",
      "Round 3\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fce543cce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fce543cce50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEval: loss=4.024, accuracy=0.168\n",
      "\tTrain: loss=3.873, accuracy=0.228\n",
      "Round 4\n",
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x7fce345429d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x7fce345429d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEval: loss=3.938, accuracy=0.157\n",
      "\tTrain: loss=3.784, accuracy=0.220\n",
      "Final evaluation\n",
      "\tEval: loss=3.830, accuracy=0.177\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 5\n",
    "\n",
    "# The state of the FL server, containing the model and optimization state.\n",
    "state = fed_avg.initialize()\n",
    "\n",
    "# Load pre-trained Keras model weights into the global model state.\n",
    "state = tff.learning.state_with_new_model_weights(\n",
    "    state,\n",
    "    trainable_weights=[v.numpy() for v in keras_model.trainable_weights],\n",
    "    non_trainable_weights=[v.numpy() for v in keras_model.non_trainable_weights])\n",
    "\n",
    "\n",
    "def keras_evaluate(state, round_num):\n",
    "  # Take  global model weights and push them back into a Keras model to\n",
    "  # use its standard `.evaluate()` method.\n",
    "  keras_model = load_model(batch_size=BATCH_SIZE)\n",
    "  keras_model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[FlattenedCategoricalAccuracy()])\n",
    "  state.model.assign_weights_to(keras_model)\n",
    "  loss, accuracy = keras_model.evaluate(example_dataset, steps=2, verbose=0)\n",
    "  print('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))\n",
    "\n",
    "\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "  print('Round {r}'.format(r=round_num))\n",
    "  keras_evaluate(state, round_num)\n",
    "  state, metrics = fed_avg.next(state, train_datasets)\n",
    "  train_metrics = metrics['train']\n",
    "  print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(\n",
    "      l=train_metrics['loss'], a=train_metrics['accuracy']))\n",
    "\n",
    "print('Final evaluation')\n",
    "keras_evaluate(state, NUM_ROUNDS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a284bf0-9fbc-40da-a37d-d20ac420c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where art thou tensorflow, you ask? en such profligates?\"\n",
      "\n",
      "\"Is it will do,\" said Monsieurshed, in a mother and the heaves\n",
      "was put in opinion, until I should have still secrete out of their dismissal from\n",
      "the side of the day. But, no\n"
     ]
    }
   ],
   "source": [
    "# Set newly trained weights back in the originally created model.\n",
    "keras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n",
    "# Text generation requires batch_size=1\n",
    "print(generate_text(keras_model_batch1, 'Where art thou tensorflow, you ask? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b03c4-2a49-47c5-b568-226a4a245d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
